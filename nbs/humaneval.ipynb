{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from human_eval.execution import check_correctness\n",
    "from human_eval.data import HUMAN_EVAL, stream_jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = list(stream_jsonl(HUMAN_EVAL))\n",
    "executor = ThreadPoolExecutor(max_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "def is_palindrome(string: str) -> bool:\n",
      "    \"\"\" Test if given string is a palindrome \"\"\"\n",
      "    return string == string[::-1]\n",
      "\n",
      "\n",
      "def make_palindrome(string: str) -> str:\n",
      "    \"\"\" Find the shortest palindrome that begins with a supplied string.\n",
      "    Algorithm idea is simple:\n",
      "    - Find the longest postfix of supplied string that is a palindrome.\n",
      "    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.\n",
      "    >>> make_palindrome('')\n",
      "    ''\n",
      "    >>> make_palindrome('cat')\n",
      "    'catac'\n",
      "    >>> make_palindrome('cata')\n",
      "    'catac'\n",
      "    \"\"\"\n",
      "\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "idx = 10\n",
    "problem = problems[idx]\n",
    "timeout = 10\n",
    "print(problem['prompt'])\n",
    "print('--')\n",
    "# print(problem['canonical_solution'])\n",
    "correct_completion = '''    if not string:\n",
    "        return ''\n",
    "\n",
    "    beginning_of_suffix = 0\n",
    "\n",
    "    while not is_palindrome(string[beginning_of_suffix:]):\n",
    "        beginning_of_suffix += 1\n",
    "\n",
    "    return string + string[:beginning_of_suffix][::-1]'''\n",
    "    \n",
    "    \n",
    "incorrect_completion = '''    return string + string[:][::-1]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: should be returning fraction of test cases passed, instead of 0/1\n",
    "def execute(problem, completion, timeout):\n",
    "    future = executor.submit(check_correctness, problem, completion, timeout)\n",
    "    result = future.result()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    if not string:\n",
      "        return ''\n",
      "\n",
      "    beginning_of_suffix = 0\n",
      "\n",
      "    while not is_palindrome(string[beginning_of_suffix:]):\n",
      "        beginning_of_suffix += 1\n",
      "\n",
      "    return string + string[:beginning_of_suffix][::-1]\n"
     ]
    }
   ],
   "source": [
    "print(correct_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = problems[3]\n",
    "completion = '''    balance = 0\n",
    "    for operation in operations:\n",
    "        balance += operation\n",
    "\n",
    "        if balance < 0:\n",
    "            return True\n",
    "\n",
    "    return False'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/3',\n",
       " 'passed': True,\n",
       " 'result': 'passed',\n",
       " 'completion_id': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute(problem, completion, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/3',\n",
       " 'passed': False,\n",
       " 'result': 'failed: name \\'string\\' is not defined, traceback: Traceback (most recent call last):\\n  File \"/Users/arunpatro/llm-mcts/human-eval/human_eval/execution.py\", line 49, in unsafe_execute\\n    exec(check_program, exec_globals)\\n  File \"<string>\", line 30, in <module>\\n    check(below_zero)\\n  File \"<string>\", line 23, in check\\n    assert candidate([]) == False\\n  File \"<string>\", line 13, in below_zero\\n    return string + string[:][::-1]\\nNameError: name \\'string\\' is not defined\\n',\n",
       " 'completion_id': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute(problem, incorrect_completion, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_execute(problem, completion, timeout):\n",
    "    pre_base_str, tests = problem['test'].split('def check(candidate):\\n')\n",
    "    base_str = \"def check(candidate):\\n\"\n",
    "    split_tests = [pre_base_str + base_str + i for i in tests.split('\\n') if i != '']\n",
    "    \n",
    "    _problem = problem.copy()\n",
    "    results = []\n",
    "    for i in split_tests:\n",
    "        _problem['test'] = i\n",
    "        future = executor.submit(check_correctness, _problem, completion, timeout)\n",
    "        result = future.result()\n",
    "        results.append(result)\n",
    "    \n",
    "    \n",
    "    return {'task_id': problem['task_id'], 'pass_rate': sum([i['passed'] for i in results])/len(results)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = '''    return False'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task_id': 'HumanEval/3', 'pass_rate': 0.5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_execute(problem, completion, timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "METADATA = {\n",
      "    'author': 'jt',\n",
      "    'dataset': 'test'\n",
      "}\n",
      "\n",
      "\n",
      "def check(candidate):\n",
      "    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2]) == (3.9, 4.0)\n",
      "    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0]) == (5.0, 5.9)\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.2]) == (2.0, 2.2)\n",
      "    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0]) == (2.0, 2.0)\n",
      "    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1]) == (2.2, 3.1)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 20\n",
    "problem = problems[idx]\n",
    "print(problem['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
